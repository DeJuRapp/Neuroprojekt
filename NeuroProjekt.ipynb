{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-tGOT28rRvg"
      },
      "source": [
        "# RBF\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pIypjUCx6VG"
      },
      "source": [
        "We choose a gauÃŸian activation function for a RBF neuron with it's derivatives $$y(x)=e^{-\\frac{||x - c||^2}{2\\sigma^2}}$$\n",
        "$$\\frac d{dc}y(x)=\\frac{x-c}{\\sigma^2}e^{-\\frac{||x - c||^2}{2\\sigma^2}}$$\n",
        "$$\\frac d{dx}y(x)=-\\frac{x-c}{\\sigma^2}e^{-\\frac{||x - c||^2}{2\\sigma^2}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The MNIST data set can be downloaded in *.CSV* [this](https://github.com/pjreddie/mnist-csv-png/blob/master/process_mnist.py) git repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RBF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Boolean Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import layer\n",
        "from neurons import RBF\n",
        "from typing import List\n",
        "import model\n",
        "\n",
        "TRAIN_RATE:float = 0.01\n",
        "TRAIN:bool = True\n",
        "FIRST_LAYER_NEURONS:int = 100\n",
        "TRAINING_REPITIONS:int = 1000\n",
        "\n",
        "layers:List[layer.DenseLayer] = []\n",
        "layers.append(layer.DenseLayer(2, FIRST_LAYER_NEURONS, RBF))\n",
        "layers.append(layer.DenseLayer(FIRST_LAYER_NEURONS, 1, RBF))\n",
        "\n",
        "m = model.Model(layers)\n",
        "\n",
        "inputs = np.array([[0.0,0.0],[1.0,0.0],[0.0,1.0],[1.0,1.0]])\n",
        "expected = np.array([0.0, 1.0, 0.0, 0.0])\n",
        "\n",
        "if TRAIN:\n",
        "    m.train(inputs, expected, TRAINING_REPITIONS)\n",
        "\n",
        "    plt.plot(m.errors, label='Error')\n",
        "    plt.plot(m.losses, label='Loss')\n",
        "    plt.legend()\n",
        "\n",
        "for i, output in enumerate(m.predict(inputs)):\n",
        "    print(f\"Output: {output[0]}\\tExpected: {expected[i]}\")\n",
        "\n",
        "pts = np.empty((FIRST_LAYER_NEURONS, 2))\n",
        "colours = np.empty((FIRST_LAYER_NEURONS,3))\n",
        "pts = layers[0].neuron_data\n",
        "colours[:] = np.array([1.0,0.0,0.0])\n",
        "plt.figure()\n",
        "plt.scatter(pts[:,0], pts[:,1],c=colours)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''Load data'''\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import Tuple\n",
        "\n",
        "def load_mnist(path:Path) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    '''\n",
        "    Returns:\n",
        "        labels, images\n",
        "    '''\n",
        "    raw_data:np.ndarray = np.loadtxt(path, delimiter=\",\")\n",
        "    indices = np.arange(raw_data.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    return raw_data[indices,0].astype(int), raw_data[indices,1:] / 255.0\n",
        "\n",
        "def create_one_hot_encoding(labels:np.ndarray)->np.ndarray:\n",
        "    '''\n",
        "    Creates a one hot encoding of the labels.\n",
        "\n",
        "    Assumes the labels start from 0.\n",
        "    '''\n",
        "    indices = np.empty((2, labels.shape[0]), dtype=int)\n",
        "    indices[0,:] = np.arange(labels.shape[0])\n",
        "    indices[1,:] = labels\n",
        "    num_labels = np.max(labels + 1)\n",
        "    encoded_labels = np.zeros((labels.shape[0], num_labels))\n",
        "    encoded_labels[indices[0], indices[1]] = 1.0\n",
        "    return encoded_labels\n",
        "\n",
        "train_labels, train_data = load_mnist(Path(\"mnist_train.csv\"))\n",
        "train_labels = create_one_hot_encoding(train_labels)\n",
        "\n",
        "validation_labels, validation_data = load_mnist((Path(\"mnist_test.csv\")))\n",
        "validation_labels = create_one_hot_encoding(validation_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''Create a model'''\n",
        "from typing import List\n",
        "import numpy as np\n",
        "import layer\n",
        "import model\n",
        "from neurons import RBF\n",
        "import loss\n",
        "\n",
        "\n",
        "FIRST_LAYER_NEURONS:int = 50\n",
        "NUM_LABELS:int = 10\n",
        "\n",
        "layers:List[layer.DenseLayer] = []\n",
        "layers.append(layer.DenseLayer(train_data.shape[1], FIRST_LAYER_NEURONS, RBF))\n",
        "layers.append(layer.DenseLayer(FIRST_LAYER_NEURONS, NUM_LABELS, RBF))\n",
        "\n",
        "m = model.Model(layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''Train the model.'''\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "TRAINING_REPITIONS:int = 50\n",
        "TRAIN_RATE:float = 0.1\n",
        "\n",
        "m.train(train_input=train_data, train_output=train_labels, epochs=TRAINING_REPITIONS, train_rate=TRAIN_RATE, loss_function=loss.quadratic_error)\n",
        "print(f\"Last error {m.errors[-1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Average validation error: {m.validate(validation_data, validation_labels)}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_correct(input:np.ndarray, expected_labels:np.ndarray):\n",
        "    output = m.predict(input)\n",
        "    labels = np.argmax(output,axis=1)\n",
        "\n",
        "    mask = expected_labels == np.argmax(labels,axis=1)\n",
        "\n",
        "    return np.sum(mask)\n",
        "\n",
        "print(f\"Classified {count_correct(train_data, train_labels)} out of {train_data.shape[0]} training examples correctly.\")\n",
        "print(f\"Classified {count_correct(validation_data, validation_labels)} out of {validation_data.shape[0]} validation examples correctly.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "yLnJEX22yzFJ"
      ],
      "name": "NeuroProjekt.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "545ce24e793e1353ad1aa1d336762d198a16bd057b21e3cc4f80d78d27aa7f07"
    },
    "kernelspec": {
      "display_name": "Python 3.7.10 64-bit ('test-env': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

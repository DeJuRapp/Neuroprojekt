{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-tGOT28rRvg"
      },
      "source": [
        "# RBF\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pIypjUCx6VG"
      },
      "source": [
        "We choose a gauÃŸian activation function for a RBF neuron with it's derivatives $$y(x)=e^{-\\sum\\limits_i\\frac{(x_i-c_i)^2}{\\sigma_i^2}}$$\n",
        "$$\\frac d{dc}y(x)=2\\sigma^2(x-c)y(x)$$\n",
        "$$\\frac d{d\\sigma}y(x)=-2(x-c)\\sigma y(x)$$\n",
        "$$\\frac d{dx}y(x)=-2\\sigma^2(x-c)y(x)$$\n",
        "\n",
        "Where $x,c,\\sigma\\in \\mathbb{R}^n$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The MNIST data set can be downloaded in *.CSV* [this](https://github.com/pjreddie/mnist-csv-png/blob/master/process_mnist.py) git repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RBF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Boolean Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import layer\n",
        "from neurons import RBF\n",
        "from typing import List\n",
        "import model\n",
        "\n",
        "TRAIN_RATE:float = 0.0001\n",
        "TRAIN:bool = True\n",
        "FIRST_LAYER_NEURONS:int = 3\n",
        "SECOND_LAYER_NEURONS:int = 6\n",
        "TRAINING_REPITIONS:int = 200\n",
        "BATCH_SIZE:int = 4\n",
        "\n",
        "layers:List[layer.DenseLayer] = []\n",
        "layers.append(layer.DenseLayer(2, FIRST_LAYER_NEURONS, RBF()))\n",
        "layers.append(layer.DenseLayer(FIRST_LAYER_NEURONS, SECOND_LAYER_NEURONS, RBF()))\n",
        "layers.append(layer.DenseLayer(SECOND_LAYER_NEURONS, 1, RBF()))\n",
        "\n",
        "m = model.Model(layers)\n",
        "\n",
        "inputs = np.array([[0.0,0.0],[1.0,0.0],[0.0,1.0],[1.0,1.0]]).reshape(-1, BATCH_SIZE, 2)\n",
        "expected = np.array([0.0, 1.0, 1.0, 0.0]).reshape(-1, BATCH_SIZE, 1)\n",
        "\n",
        "if TRAIN:\n",
        "    \n",
        "    m.train(inputs, expected, TRAINING_REPITIONS)\n",
        "\n",
        "    plt.plot(m.losses, label='Loss')\n",
        "    plt.legend()\n",
        "\n",
        "predicted_output = m.predict(inputs.reshape(1, 4, 2))\n",
        "\n",
        "for i, output in enumerate(predicted_output.reshape(-1, 1)):\n",
        "    print(f\"Output: {output}\\tExpected: {expected.flatten()[i]}\")\n",
        "\n",
        "pts = np.empty((FIRST_LAYER_NEURONS, 2))\n",
        "colours = np.empty((FIRST_LAYER_NEURONS,3))\n",
        "pts = layers[0].neurons.c\n",
        "colours[:] = np.array([1.0,0.0,0.0])\n",
        "plt.figure()\n",
        "plt.scatter(pts[:,0], pts[:,1],c=colours)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "m.predict(inputs.reshape(1, 4, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Results\n",
        "\n",
        "* Even if the deviation can be trained, choosing a deviation that is too large, will result in odd behaviour.\n",
        "\n",
        "  To give an example, if we're trying to learn the _AND_ function, the output will contain more zeros than ones. Using gradient decent it is better to decrease the error by creating many zeros which is done by moving away the center instead of lowering the deviation. In testing, this often results in getting in a state which is badly trained, but we cannot get out of.\n",
        "\n",
        "* Training RBFs with multiple dense layers is less stable and will not converge as reliably using gradient decent. I believe this happens due to each RBF of lower levels 'prefering' a     different representation of the underlying feature space, and therefore no stable state is reached."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### First test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''Load data'''\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import Tuple\n",
        "\n",
        "BATCH_SIZE: int = 200\n",
        "\n",
        "def load_mnist(path:Path) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    '''\n",
        "    Returns:\n",
        "        labels, images\n",
        "    '''\n",
        "    raw_data:np.ndarray = np.loadtxt(path, delimiter=\",\")\n",
        "    indices = np.arange(raw_data.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    return raw_data[indices,0].astype(int), raw_data[indices,1:] / 255.0\n",
        "\n",
        "def filter_mnist(data, labels, filter_id):\n",
        "    indices = np.where(labels==filter_id)\n",
        "    return data[indices], labels[indices]\n",
        "\n",
        "def create_one_hot_encoding(labels:np.ndarray)->np.ndarray:\n",
        "    '''\n",
        "    Creates a one hot encoding of the labels.\n",
        "\n",
        "    Assumes the labels start from 0.\n",
        "    '''\n",
        "    indices = np.empty((2, labels.shape[0]), dtype=int)\n",
        "    indices[0,:] = np.arange(labels.shape[0])\n",
        "    indices[1,:] = labels\n",
        "    num_labels = np.max(labels + 1)\n",
        "    encoded_labels = np.zeros((labels.shape[0], num_labels))\n",
        "    encoded_labels[indices[0], indices[1]] = 1.0\n",
        "    #encoded_labels = np.where(labels == 0, 1.0, 0.0).reshape(-1,1)\n",
        "    return encoded_labels\n",
        "\n",
        "def create_batches(data:np.ndarray, labels:np.ndarray, batch_size:int) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    return data.reshape(-1, batch_size, data.shape[1]), labels.reshape(-1, batch_size, labels.shape[1])\n",
        "\n",
        "train_labels, train_data = load_mnist(Path(\"mnist_train.csv\"))\n",
        "#train_data, train_labels = filter_mnist(train_data, train_labels, 0)\n",
        "train_labels = create_one_hot_encoding(train_labels)\n",
        "\n",
        "validation_labels, validation_data = load_mnist((Path(\"mnist_test.csv\")))\n",
        "validation_labels = create_one_hot_encoding(validation_labels)\n",
        "\n",
        "train_data, train_labels = create_batches(train_data, train_labels, BATCH_SIZE)\n",
        "validation_data, validation_labels = create_batches(validation_data, validation_labels, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''Create a model'''\n",
        "from typing import List\n",
        "import numpy as np\n",
        "import layer\n",
        "import model\n",
        "from neurons import RBF\n",
        "import loss\n",
        "\n",
        "FIRST_LAYER_NEURONS:int = 512\n",
        "SECOND_LAYER_NEURONS:int = 512\n",
        "NUM_LABELS:int = train_labels.shape[2]\n",
        "\n",
        "layers:List[layer.DenseLayer] = []\n",
        "layers.append(layer.DenseLayer(train_data.shape[2], FIRST_LAYER_NEURONS, RBF()))\n",
        "layers.append(layer.DenseLayer(FIRST_LAYER_NEURONS, SECOND_LAYER_NEURONS, RBF()))\n",
        "layers.append(layer.DenseLayer(SECOND_LAYER_NEURONS, NUM_LABELS, RBF()))\n",
        "\n",
        "m = model.Model(layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''Train the model.'''\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "TRAINING_REPITIONS:int = 1\n",
        "TRAIN_RATE:float = 0.0001\n",
        "\n",
        "m.train(train_input=train_data, train_output=train_labels, epochs=TRAINING_REPITIONS, train_rate=TRAIN_RATE, loss_function=loss.quadratic_error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(np.array(m.losses).flatten(), label=\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Average validation error: {m.validate(validation_data, validation_labels)}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_correct(input:np.ndarray, expected_labels:np.ndarray):\n",
        "    output = m.predict(input).reshape(-1, 10)\n",
        "    labels = np.argmax(output,axis=1)\n",
        "\n",
        "\n",
        "    mask = labels == np.argmax(expected_labels.reshape(-1,10),axis=1)\n",
        "\n",
        "    return np.sum(mask)\n",
        "    #return np.sum(np.abs(np.where(m.predict(input).flatten() >= 0.5, 1.0, 0.0) - expected_labels.flatten()))\n",
        "\n",
        "#print(f\"Classified {count_correct(train_data, train_labels)} out of {train_data.shape[0] * train_data.shape[1]} training examples correctly.\")\n",
        "print(f\"Classified {count_correct(validation_data, validation_labels)} out of {validation_data.shape[0] * validation_data.shape[1]} validation examples correctly.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def visualize_prototypes(rbf_weights:np.ndarray):\n",
        "    grey_scale_image = np.empty((*rbf_weights.shape, 3))\n",
        "    grey_scale_image[:,0] = rbf_weights\n",
        "    grey_scale_image[:,1] = rbf_weights\n",
        "    grey_scale_image[:,2] = rbf_weights\n",
        "    plt.figure()\n",
        "    plt.imshow(grey_scale_image.reshape(28, 28, 3))\n",
        "    plt.show()\n",
        "\n",
        "visualize_prototypes(m.layers[0].neurons.c[1])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "yLnJEX22yzFJ"
      ],
      "name": "NeuroProjekt.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "545ce24e793e1353ad1aa1d336762d198a16bd057b21e3cc4f80d78d27aa7f07"
    },
    "kernelspec": {
      "display_name": "Python 3.7.10 64-bit ('test-env': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

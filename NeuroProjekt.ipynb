{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-tGOT28rRvg"
      },
      "source": [
        "# RBF\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pIypjUCx6VG"
      },
      "source": [
        "We choose a gauÃŸian activation function for a RBF neuron with it's derivatives $$y(x)=e^{-\\sum\\limits_i\\frac{(x_i-c_i)^2}{\\sigma_i^2}}$$\n",
        "$$\\frac d{dc}y(x)=2\\sigma^2(x-c)y(x)$$\n",
        "$$\\frac d{d\\sigma}y(x)=-2(x-c)\\sigma y(x)$$\n",
        "$$\\frac d{dx}y(x)=-2\\sigma^2(x-c)y(x)$$\n",
        "\n",
        "Where $x,c,\\sigma\\in \\mathbb{R}^n$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The MNIST data set can be downloaded in *.CSV* [this](https://github.com/pjreddie/mnist-csv-png/blob/master/process_mnist.py) git repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RBF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Boolean Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import layer\n",
        "from neurons import RBF\n",
        "from typing import List\n",
        "import model\n",
        "\n",
        "TRAIN_RATE:float = 0.0001\n",
        "TRAIN:bool = True\n",
        "FIRST_LAYER_NEURONS:int = 5\n",
        "SECOND_LAYER_NEURONS:int = 10\n",
        "NUM_LABELS:int = 2\n",
        "TRAINING_REPITIONS:int = 100\n",
        "BATCH_SIZE:int = 4\n",
        "\n",
        "\n",
        "inputs = np.array([[0.0,0.0],[1.0,0.0],[0.0,1.0],[1.0,1.0]]).reshape(-1, BATCH_SIZE, 2)\n",
        "expected = np.array([[1.0, 0.0], [1.0,0.0], [0.0,1.0], [0.0,1.0]]).reshape(-1, BATCH_SIZE, NUM_LABELS)\n",
        "\n",
        "layers:List[layer.DenseLayer] = []\n",
        "layers.append(layer.DenseLayer(2, FIRST_LAYER_NEURONS, RBF()))\n",
        "#features = layers[-1].subsample_EM_init(inputs.reshape(-1,2)[:min(FIRST_LAYER_NEURONS, 4)], expected.reshape(-1,NUM_LABELS)[:min(FIRST_LAYER_NEURONS, 4)])\n",
        "layers.append(layer.DenseLayer(FIRST_LAYER_NEURONS, SECOND_LAYER_NEURONS, RBF()))\n",
        "#features = layers[-1].subsample_EM_init(features[:min(FIRST_LAYER_NEURONS, SECOND_LAYER_NEURONS)], expected.reshape(-1,NUM_LABELS)[:min(FIRST_LAYER_NEURONS, SECOND_LAYER_NEURONS)])\n",
        "layers.append(layer.DenseLayer(SECOND_LAYER_NEURONS, NUM_LABELS, RBF()))\n",
        "#features = layers[-1].subsample_EM_init(features[:min(NUM_LABELS, SECOND_LAYER_NEURONS)], expected.reshape(-1,NUM_LABELS)[:min(NUM_LABELS, SECOND_LAYER_NEURONS)])\n",
        "\n",
        "m = model.Model(layers)\n",
        "\n",
        "original_c = layers[0].neurons.c\n",
        "\n",
        "if TRAIN:\n",
        "    \n",
        "    m.train(inputs, expected, TRAINING_REPITIONS)\n",
        "\n",
        "    plt.plot(m.losses, label='Loss')\n",
        "    plt.legend()\n",
        "\n",
        "predicted_output = m.predict(inputs.reshape(1, 4, 2))\n",
        "\n",
        "for i, output in enumerate(predicted_output.reshape(-1, NUM_LABELS)):\n",
        "    out_string = \"\"\n",
        "    for j in range(NUM_LABELS):\n",
        "        out_string += f\"{predicted_output[0,i,j]:.2f}, \"\n",
        "\n",
        "    print(f\"Output: [{out_string}]\\tExpected: {expected.reshape(-1, NUM_LABELS)[i]}\")\n",
        "\n",
        "pts = np.empty((FIRST_LAYER_NEURONS, 2))\n",
        "colours = np.empty((FIRST_LAYER_NEURONS,3))\n",
        "pts = layers[0].neurons.c\n",
        "colours[:] = np.array([0.0,0.0,1.0])\n",
        "plt.figure()\n",
        "plt.scatter(original_c[:,0], original_c[:,1],c=colours)\n",
        "colours[:] = np.array([1.0,0.0,0.0])\n",
        "plt.scatter(pts[:,0], pts[:,1],c=colours)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "m.predict(inputs.reshape(1, 4, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Results\n",
        "\n",
        "* Even if the deviation can be trained, choosing a deviation that is too large, will result in odd behaviour.\n",
        "\n",
        "  To give an example, if we're trying to learn the _AND_ function, the output will contain more zeros than ones. Using gradient decent it is better to decrease the error by creating many zeros which is done by moving away the center instead of lowering the deviation. In testing, this often results in getting in a state which is badly trained, but we cannot get out of.\n",
        "\n",
        "* Training RBFs with multiple dense layers is less stable and will not converge as reliably using gradient decent. I believe this happens due to each RBF of lower levels 'prefering' a     different representation of the underlying feature space, and therefore no stable state is reached."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### First test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''Load data functions'''\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import Tuple\n",
        "import pickle\n",
        "\n",
        "def load_mnist_csv(path:Path) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    '''\n",
        "    Returns:\n",
        "        labels, images\n",
        "    '''\n",
        "    raw_data:np.ndarray = np.loadtxt(path, delimiter=\",\")\n",
        "    indices = np.arange(raw_data.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    return raw_data[indices,0].astype(int), raw_data[indices,1:] / 255.0\n",
        "\n",
        "def load_mnist_pickle(path:Path) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    with open(path, \"rb\") as f:\n",
        "        data = pickle.load(f)\n",
        "    return data[\"labels\"], data[\"data\"]\n",
        "\n",
        "def filter_mnist(data, labels, filter_id):\n",
        "    indices = np.where(labels==filter_id)\n",
        "    return data[indices], labels[indices]\n",
        "\n",
        "def create_one_hot_encoding(labels:np.ndarray)->np.ndarray:\n",
        "    '''\n",
        "    Creates a one hot encoding of the labels.\n",
        "\n",
        "    Assumes the labels start from 0.\n",
        "    '''\n",
        "    indices = np.empty((2, labels.shape[0]), dtype=int)\n",
        "    indices[0,:] = np.arange(labels.shape[0])\n",
        "    indices[1,:] = labels\n",
        "    num_labels = np.max(labels + 1)\n",
        "    encoded_labels = np.zeros((labels.shape[0], num_labels))\n",
        "    encoded_labels[indices[0], indices[1]] = 1.0\n",
        "    #encoded_labels = np.where(labels == 0, 1.0, 0.0).reshape(-1,1)\n",
        "    return encoded_labels\n",
        "\n",
        "def create_batches(data:np.ndarray, labels:np.ndarray, batch_size:int) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    return data.reshape(-1, batch_size, data.shape[1]), labels.reshape(-1, batch_size, labels.shape[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''Load pickle data'''\n",
        "BATCH_SIZE: int = 500\n",
        "\n",
        "#Binary pickle files load much faster than the csv\n",
        "train_labels, train_data = load_mnist_pickle(Path(\"mnist_train.pickle\"))\n",
        "train_labels = create_one_hot_encoding(train_labels)\n",
        "\n",
        "validation_labels, validation_data = load_mnist_pickle(Path(\"mnist_test.pickle\"))\n",
        "validation_labels = create_one_hot_encoding(validation_labels)\n",
        "\n",
        "train_data, train_labels = create_batches(train_data, train_labels, BATCH_SIZE)\n",
        "validation_data, validation_labels = create_batches(validation_data, validation_labels, 1000)\n",
        "\n",
        "train_data = train_data.reshape(-1, BATCH_SIZE, 28, 28)\n",
        "validation_data = validation_data.reshape(-1, 1000, 28, 28)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''Load csv data'''\n",
        "BATCH_SIZE: int = 50\n",
        "\n",
        "train_labels, train_data = load_mnist_csv(Path(\"mnist_train.csv\"))\n",
        "train_labels = create_one_hot_encoding(train_labels)\n",
        "\n",
        "validation_labels, validation_data = load_mnist_csv(Path(\"mnist_test.csv\"))\n",
        "validation_labels = create_one_hot_encoding(validation_labels)\n",
        "\n",
        "train_data, train_labels = create_batches(train_data, train_labels, BATCH_SIZE)\n",
        "validation_data, validation_labels = create_batches(validation_data, validation_labels, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''Convert csv to pickle data'''\n",
        "import pickle\n",
        "\n",
        "\n",
        "train_labels, train_data = load_mnist_csv(Path(\"mnist_train.csv\"))\n",
        "with open(\"mnist_train.pickle\", \"wb\") as f:\n",
        "    pickle.dump({\"data\": train_data, \"labels\": train_labels}, f)\n",
        "\n",
        "validation_labels, validation_data = load_mnist_csv(Path(\"mnist_test.csv\"))\n",
        "with open(\"mnist_test.pickle\", \"wb\") as f:\n",
        "    pickle.dump({\"data\": validation_data, \"labels\": validation_labels}, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Model creation and training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''Create a model'''\n",
        "from typing import List\n",
        "import numpy as np\n",
        "import layer\n",
        "import model\n",
        "from neurons import RBF\n",
        "import loss\n",
        "\n",
        "FIRST_LAYER_NEURONS:int = 1000\n",
        "SECOND_LAYER_NEURONS:int = 1200\n",
        "NUM_LABELS:int = train_labels.shape[2]\n",
        "\n",
        "layers:List[layer.DenseLayer] = []\n",
        "layers.append(layer.DenseLayer(train_data.shape[2], FIRST_LAYER_NEURONS, RBF()))\n",
        "layers.append(layer.DenseLayer(FIRST_LAYER_NEURONS, SECOND_LAYER_NEURONS, RBF()))\n",
        "layers.append(layer.DenseLayer(SECOND_LAYER_NEURONS, NUM_LABELS, RBF()))\n",
        "\n",
        "m = model.Model(layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''Create a model with smaller input regions'''\n",
        "from typing import List\n",
        "import numpy as np\n",
        "import layer\n",
        "import model\n",
        "from neurons import RBF\n",
        "import loss\n",
        "\n",
        "FIRST_LAYER_NEURONS:int = 25\n",
        "SECOND_LAYER_NEURONS:int = 20\n",
        "NUM_LABELS:int = train_labels.shape[2]\n",
        "\n",
        "train_data = train_data.reshape(-1, BATCH_SIZE, 28, 28)\n",
        "validation_data = validation_data.reshape(-1, BATCH_SIZE, 28, 28)\n",
        "\n",
        "layers:List[layer.DenseLayer] = []\n",
        "layers.append(layer.ConvolutionalLayer(train_data.shape, 2, 5, RBF()))\n",
        "lay: layer.ConvolutionalLayer = layers[-1]\n",
        "layers.append(layer.ConvolutionalLayer((1, BATCH_SIZE, lay.num_horizontal, lay.num_vertical), 2, 5, RBF()))\n",
        "lay: layer.ConvolutionalLayer = layers[-1]\n",
        "#layers.append(layer.ConvolutionalLayer((1, BATCH_SIZE, lay.num_horizontal, lay.num_vertical), 2, 3, RBF()))\n",
        "#lay: layer.ConvolutionalLayer = layers[-1]\n",
        "#layers.append(layer.DenseLayer(lay.num_horizontal * lay.num_vertical, FIRST_LAYER_NEURONS, RBF()))\n",
        "#layers.append(layer.DenseLayer(FIRST_LAYER_NEURONS, SECOND_LAYER_NEURONS, RBF()))\n",
        "#layers.append(layer.DenseLayer(SECOND_LAYER_NEURONS, NUM_LABELS, RBF()))\n",
        "layers.append(layer.DenseLayer(lay.num_horizontal * lay.num_vertical, NUM_LABELS, RBF()))\n",
        "\n",
        "m = model.Model(layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''Train the model.'''\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "TRAINING_REPITIONS:int = 100\n",
        "TRAIN_RATE:float = 0.001\n",
        "\n",
        "m.train(train_input=train_data, train_output=train_labels, epochs=TRAINING_REPITIONS, train_rate=TRAIN_RATE, loss_function=loss.quadratic_error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(np.array(m.losses), label=\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_correct(input:np.ndarray, expected_labels:np.ndarray):\n",
        "    output = m.predict(input).reshape(-1, 10)\n",
        "    labels = np.argmax(output,axis=1)\n",
        "\n",
        "\n",
        "    mask = labels == np.argmax(expected_labels.reshape(-1,10),axis=1)\n",
        "\n",
        "    return np.sum(mask)\n",
        "    #return np.sum(np.abs(np.where(m.predict(input).flatten() >= 0.5, 1.0, 0.0) - expected_labels.flatten()))\n",
        "\n",
        "print(f\"Classified {count_correct(train_data, train_labels)} out of {train_data.shape[0] * train_data.shape[1]} training examples correctly.\")\n",
        "print(f\"Classified {count_correct(validation_data, validation_labels)} out of {validation_data.shape[0] * validation_data.shape[1]} validation examples correctly.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def visualize_prototypes(rbf_weights:np.ndarray):\n",
        "    grey_scale_image = np.empty((*rbf_weights.shape, 3))\n",
        "    grey_scale_image[:,0] = rbf_weights\n",
        "    grey_scale_image[:,1] = rbf_weights\n",
        "    grey_scale_image[:,2] = rbf_weights\n",
        "    plt.figure()\n",
        "    plt.imshow(grey_scale_image.reshape(28, 28, 3))\n",
        "    plt.show()\n",
        "\n",
        "visualize_prototypes(m.layers[0].neurons.c[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import serialize\n",
        "\n",
        "serialize.serialize_model(m, \"model.pickle\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results show that these large dimensions heavly suffer from the vanishing gradient problem.\n",
        "\n",
        "Two ideas I have for solving this are:\n",
        "* BatchNorm layer\n",
        "    * I believe this may be counter productive for learning as we generally want to increase the diversity and not normalize this.\n",
        "    Although this may work in our favour since the network may be forced to create representations where the centers of a label are forced into different directions instead of just creating spacial distance.\n",
        "* Implement RPROP, which suffers less from vanishing gradient as it only considers the behaviour relating towards the sign of a gradient vector.\n",
        "* Make a \"fractal\" scheme where each neuron only learns a small dimensional subimage."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "yLnJEX22yzFJ"
      ],
      "name": "NeuroProjekt.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "545ce24e793e1353ad1aa1d336762d198a16bd057b21e3cc4f80d78d27aa7f07"
    },
    "kernelspec": {
      "display_name": "Python 3.7.10 64-bit ('test-env': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
